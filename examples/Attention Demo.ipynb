{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T06:20:59.895000Z",
     "start_time": "2018-06-15T06:20:59.888000Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Layer, LSTM, TimeDistributed\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "# import keras-layer-zoo from parent directory\n",
    "import sys\n",
    "sys.path.append(\"./..\")\n",
    "from kulc import attention, layer_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:15:58.008800Z",
     "start_time": "2018-06-13T15:15:58.006800Z"
    }
   },
   "source": [
    "# Demos\n",
    "\n",
    "This notebook contains some very rudimentary examples how to use (some of) the layers implemented in the *Keras Utility & Layer Collection (kulc)*.\n",
    "\n",
    "- [Scaled Dot-Product Attention](#sdpattention)\n",
    "- [Multi-Head Attention](#mhatn)\n",
    "- [Layer Normalization](#layernorm)\n",
    "- [Sequencewise Attention](#seqatn)\n",
    "- [Attention Wrapper](#atnwrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention <a name=\"sdpattention\"></a>\n",
    "\n",
    "Implementation as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762). Performs a non-linear transformation on the values `V` by comparing the queries `Q` with the keys `K`. The illustration below is taken from the paper cited above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T06:21:06.128000Z",
     "start_time": "2018-06-15T06:21:01.171000Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# input: time series with 16 steps\n",
    "# each step has a 256dim valuethe output sequences\n",
    "# of a LSTM, RNN, etc.\n",
    "net_input = Input(shape=(16, 256))\n",
    "net = TimeDistributed(Dense(256))(net_input)\n",
    "\n",
    "# queries\n",
    "net_q = TimeDistributed(Dense(256))(net_input)\n",
    "# values\n",
    "net_v = TimeDistributed(Dense(256))(net_input)\n",
    "# keys\n",
    "net_k = TimeDistributed(Dense(256))(net_input)\n",
    "\n",
    "# add one ScaledDotProductAttention layer\n",
    "net = attention.ScaledDotProductAttention(name=\"attention\", return_attention=False)([net_q, net_v, net_k])\n",
    "\n",
    "net_output = TimeDistributed(Dense(128))(net)\n",
    "\n",
    "model = Model(inputs=net_input, outputs=net_output)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# dummy data\n",
    "x = np.random.rand(64, 16, 256)\n",
    "y = np.random.rand(64, 16, 128)\n",
    "\n",
    "model.fit(x, y, batch_size=16, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention <a name=\"mhatn\"></a>\n",
    "Implementation as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762). This is basically just a bunch a [Scaled Dot-Product Attention](#sdpattention) blocks whose output is combined with a linear transformation. The illustration below is taken from the paper cited above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:47:41.866903Z",
     "start_time": "2018-06-13T15:47:38.037703Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input: time series with 16 steps\n",
    "# each step has a 256dim valuethe output sequences\n",
    "# of a LSTM, RNN, etc.\n",
    "net_input = Input(shape=(16, 256))\n",
    "net = TimeDistributed(Dense(256))(net_input)\n",
    "\n",
    "# queries\n",
    "net_q = TimeDistributed(Dense(256))(net_input)\n",
    "# values\n",
    "net_v = TimeDistributed(Dense(256))(net_input)\n",
    "# keys\n",
    "net_k = TimeDistributed(Dense(256))(net_input)\n",
    "\n",
    "# h: the number of parallel attention heads\n",
    "net = attention.MultiHeadAttention(h=2, name=\"attention\", return_attention=False)([net_q, net_v, net_k])\n",
    "\n",
    "net_output = TimeDistributed(Dense(128))(net)\n",
    "\n",
    "model = Model(inputs=net_input, outputs=net_output)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# dummy data\n",
    "x = np.random.rand(64, 16, 256)\n",
    "y = np.random.rand(64, 16, 128)\n",
    "\n",
    "model.fit(x, y, batch_size=16, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization <a name=\"layernorm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:38:37.278601Z",
     "start_time": "2018-06-13T15:38:36.100501Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# input: time series with 16 steps\n",
    "# each step has a 256dim valuethe output sequences\n",
    "# of a LSTM, RNN, etc.\n",
    "net_input = Input(shape=(16, 256))\n",
    "net = TimeDistributed(Dense(128))(net_input)\n",
    "\n",
    "net_output = layer_normalization.LayerNormalization(name=\"normalization\")(net)\n",
    "\n",
    "model = Model(inputs=net_input, outputs=net_output)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# dummy data\n",
    "x = np.random.rand(64, 16, 256)\n",
    "y = np.random.rand(64, 16, 128)\n",
    "\n",
    "model.fit(x, y, batch_size=16, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencewise Attention <a name=\"seqatn\"></a>\n",
    "\n",
    "This layer applies various attention transformations on data. It needs a time-series of queries and a time-series of values to calculate the attention and the final linear transformation to obtain the output. This is a faster version of the general attention technique. It is similar to the global attention method described in [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "\n",
    "It takes two inputs of the shape (batch_size, T, dim1) and (batch_size, T, dim2),\n",
    "whereby the first item is the source data and the second one the key data.\n",
    "This layer then calculates for each batch's element and each time step a softmax attention \n",
    "vector between the key data and the source data. Finally, this attention vector is multiplied\n",
    "with the source data to obtain a weighted output. This means, that the key data is used to\n",
    "interpret the source data in a special way to create an output of the same shape as the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T15:41:35.879201Z",
     "start_time": "2018-06-13T15:41:29.216001Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# extension of the Seq2Seq example from:\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "latent_dim = 256\n",
    "n_encoder_tokens = 16\n",
    "n_decoder_tokens = 16\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens), name=\"encoder_inputs\")\n",
    "encoder = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"encoder\")\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens), name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(n_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# The follwing two lines of code are the only difference to the tutorial's code.\n",
    "# They add a sequencewise attention layer to the model.\n",
    "\n",
    "# The layer performs on the encoder state sequence and the\n",
    "# decoder state sequence.\n",
    "# It takes two inputs of the shape (batch_size, T, dim1) and (batch_size, T, dim2),\n",
    "# whereby the first item is the source data and the second one the key data.\n",
    "# This layer then calculates for each batch's element and each time step a softmax attention \n",
    "# vector between the key data and the source data. Finally, this attention vector is multiplied\n",
    "# with the source data to obtain a weighted output. This means, that the key data is used to\n",
    "# interpret the source data in a special way to create an output of the same shape as the source data.\n",
    "decoder_attention = attention.SequenceAttention(similarity=\"additive\", name=\"attention\")\n",
    "decoder_outputs = decoder_attention([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# dummy data\n",
    "x_encoder = np.random.randn(64, 20, n_encoder_tokens)\n",
    "x_decoder = np.random.randn(64, 20, n_encoder_tokens)\n",
    "y = np.random.randn(64, 20, n_decoder_tokens)\n",
    "\n",
    "model.fit(x=[x_encoder, x_decoder], y=y, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T16:09:00.154309Z",
     "start_time": "2018-06-13T16:09:00.138709Z"
    }
   },
   "source": [
    "### Attention Wrapper/Input Feeding Attention <a name=\"atnwrapper\"></a>\n",
    "The idea of the implementation is based on the paper:\n",
    "    [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) by Luong et al.\n",
    "\n",
    "This layer is an attention layer, which can be wrapped around arbitrary RNN layers.\n",
    "This way, after each time step an attention vector is calculated\n",
    "based on the current output of the LSTM and the entire input time series.\n",
    "This attention vector is then used as a weight vector to choose special values\n",
    "from the input data. This data is then finally concatenated to the next input\n",
    "time step's data. On this a linear transformation in the same space as the input data's space\n",
    "is performed before the data is fed into the RNN cell again.\n",
    "\n",
    "This technique is similar to the *input-feeding* method described in the paper cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T16:01:36.045607Z",
     "start_time": "2018-06-13T16:01:30.111507Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# extension of the Seq2Seq example from:\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "latent_dim = 256\n",
    "n_encoder_tokens = 16\n",
    "n_decoder_tokens = 16\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, n_encoder_tokens), name=\"encoder_inputs\")\n",
    "encoder = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"encoder\")\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, n_decoder_tokens), name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_pure_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder\")\n",
    "\n",
    "# This line is the only difference compared to the Seq2Seq example from the Keras blog.\n",
    "# It adds a attention layer, which is wrapped around the decoder LSTM.\n",
    "# This way, after each time step an attention vector is calculated\n",
    "# based on the current output of the LSTM and the entire input time series.\n",
    "# This attention vector is then used as a weight vector to choose special values\n",
    "# from the input data. This data is then finally concatenated to the next input\n",
    "# time step's data. On this a linear transformation in the same space as the input data's space\n",
    "# is performed before the data is fed into the RNN cell again.\n",
    "decoder_lstm = attention.AttentionRNNWrapper(decoder_pure_lstm)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(n_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# dummy data\n",
    "x_encoder = np.random.randn(64, 20, n_encoder_tokens)\n",
    "x_decoder = np.random.randn(64, 20, n_encoder_tokens)\n",
    "y = np.random.randn(64, 20, n_decoder_tokens)\n",
    "\n",
    "model.fit(x=[x_encoder, x_decoder], y=y, batch_size=32, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
